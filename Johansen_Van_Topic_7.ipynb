{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will be addressing one application of recurrent neural networks (RNN), implementing in Python a multi-layer RNN for training/sampling from character-level language models. The model takes one text file as input and trains an RNN to learn to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. The echo state network is addressed, as well as long short-term memory and gate recurrent units.\n",
    "\n",
    "Get the available code “min-char-rnn.py” and “grad-check.py”. Using any IDE environment of your choosing, copy and paste the code. Download the character-level RNN, \"karpathy/char-rnn,\" located in the Topic 7 Study Materials, and train it on the “tiny Shakespeare” data set available at the same location. Consider the following:\n",
    "\n",
    "1. What is the RNN architecture used for reading the code “min-char-rnn.py”?\n",
    "2. Create outputs of the language model after training for 5 epochs.\n",
    "3. Create outputs of the language model after training for 50 epochs.\n",
    "4. Create outputs of the language model after training for 500 epochs.\n",
    "5. Check the gradient descent using the code “grad-check.py”.\n",
    "6. What significant differences do you see between the three outputs?\n",
    "7. What are the challenges encountered for training the RNN?\n",
    "\n",
    "The Python code necessary for this assignment is available within the zip file \"DSC-550-Python-Project-Files,\" located in the Course Materials.\n",
    "\n",
    "APA style is not required, but solid academic writing is expected. \n",
    "\n",
    "This assignment uses a rubric. Please review the rubric prior to beginning the assignment to become familiar with the expectations for successful completion. \n",
    "\n",
    "Submit a Microsoft Word document with the source code, as directed by your instructor.\n",
    "\n",
    "Zip your document and submit to LoudCloud or GitHub, as directed by your instructor. Be sure to include the GitHub link in your Word document if submitting via GitHub.\n",
    "\n",
    "You are required to submit this assignment to LopesWrite. Refer to the LopesWrite Technical Support articles for assistance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checking\n",
    "from random import uniform\n",
    "\n",
    "def gradCheck(inputs, target, hprev):\n",
    "\n",
    "    global Wxh, Whh, Why, bh, by\n",
    "    num_checks, delta = 10, 1e-5\n",
    "    _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)\n",
    "    for param,dparam,name in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], ['Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
    "        s0 = dparam.shape\n",
    "        s1 = param.shape\n",
    "        assert s0 == s1\n",
    "#     assert s0 == s1, 'Error dims dont match: %s and %s.' % (`s0`, `s1`)\n",
    "        print(name)\n",
    "    for i in range(num_checks):\n",
    "        ri = int(uniform(0,param.size))\n",
    "        # evaluate cost at [x + delta] and [x - delta]\n",
    "        old_val = param.flat[ri]\n",
    "        param.flat[ri] = old_val + delta\n",
    "        cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "        param.flat[ri] = old_val - delta\n",
    "        cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "        param.flat[ri] = old_val # reset old value for this parameter\n",
    "        # fetch both numerical and analytic gradient\n",
    "        grad_analytic = dparam.flat[ri]\n",
    "        grad_numerical = (cg0 - cg1) / ( 2 * delta )\n",
    "        rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "        print('%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))\n",
    "#         print(rel_error)\n",
    "        # rel_error should be on order of 1e-7 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n",
      "GRADIENT CHECK: \n",
      "Wxh\n",
      "Whh\n",
      "Why\n",
      "bh\n",
      "by\n",
      "0.384685, 0.384685 => 2.888849e-09 \n",
      "0.384810, 0.384810 => 1.732522e-09 \n",
      "0.384495, 0.384495 => 1.075574e-09 \n",
      "0.384583, 0.384583 => 1.141845e-09 \n",
      "0.384420, 0.384420 => 2.233834e-09 \n",
      "0.384710, 0.384710 => 6.889657e-10 \n",
      "0.384598, 0.384598 => 1.290958e-09 \n",
      "0.384648, 0.384648 => 8.646096e-10 \n",
      "0.384666, 0.384666 => 2.762385e-10 \n",
      "0.384896, 0.384896 => 3.300201e-09 \n",
      "None\n",
      "----\n",
      " vh wdhdhrh l ajl l h a l l hvl lololdGol a hdlvl ldl h l c l l a adhoh ldhohoEdldavaol loldOoholkh h VoL h hdholoh l hva h hoh holoPdl hoadl l l l l Yehvlda lvlvldLdldldldlolvlolo: ldiolol loldholvlol \n",
      "----\n",
      "iter 5, loss: 104.748499\n",
      "----\n",
      " nyedefo; d veyev ytdltiyezsheoef\n",
      " ka  rn u,keutOe\n",
      "ndeue ralR arrjweoeueu;;ib?aidiItcncmyebedec  ebluocefiuta afnhdeo d neyes ots\n",
      "Eed t ;soi!tpA;esg  ;ey .td zebennu M 'ezeu dlue'IfeOso ; ; iesBoi; yew \n",
      "----\n",
      "iter 50, loss: 105.741687\n",
      "----\n",
      " iis mdtrty tal !\n",
      "wh m  lmiticiui,s  yi thime iauCl\n",
      "yUOhiSr\n",
      "u O Omecols z nlnublOsnnsegrOririrsh aOly achet iughehnool fet, n\n",
      "Ssag Csbolnr \n",
      "Bl halnd rra sbli tht'rwtae\n",
      "\n",
      "Xir,he to rof,\n",
      "Ohi yua\n",
      "\n",
      "TOhOt ve \n",
      "----\n",
      "iter 500, loss: 97.024087\n",
      "----\n",
      " e the\n",
      "\n",
      "n heour gor tee,\n",
      "Tu gave co the trit;\n",
      "S.\n",
      "\n",
      "With asds ie fe whath wrant woake\n",
      "fole awld hak stcoper\n",
      "'fthater!\n",
      "HA\n",
      "Af ou cive diths\n",
      "Sainet an the sing Knetrel, worl sinp you soily: lerils no climoy \n",
      "----\n",
      "iter 5000, loss: 55.931717\n",
      "----\n",
      " t doth. Refent ane: 'tames: fous-\n",
      "not 'to it sourne lleray\n",
      "Make yith the wenew, Is ffrme's, ane, you with int.\n",
      "He reenevand tuin; have t upiritionon.\n",
      "\n",
      "Shak,\n",
      "Dout I fall vist buther: deim\n",
      "I town Ford,  \n",
      "----\n",
      "iter 50000, loss: 48.412287\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "print('GRADIENT CHECK: ')\n",
    "print(gradCheck(inputs, targets, hprev))\n",
    "    \n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n == 5 or n == 50 or n == 500 or n == 5000 or n == 50000:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n == 5 or n == 50 or n == 500 or n == 5000 or n == 50000: \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\n",
    "#         print('iter %d, Grad: %f' % (n, gradCheck(inputs, targets, hprev)))  # print grad\n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter \n",
    "    if n == 50001:\n",
    "        break\n",
    "    \n",
    "#     if n == 5 or n == 50 or n == 500 or n == 5000 or n == 50000 or n == 500000: \n",
    "#         print('iter %d, grad output: %f' % (n, gradCheck(inputs, targets, hprev)))  # print grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the Gradient descentoutput, it appears that one of the most common challenges with RNNs has occured in this analysis as well - there is vanishing gradients. When training a vanilla RNN using back-propagation, the gradients which are back-propagated can vanish or explode(0 or infinity) because of the computations involved in the process, which use finite-precision numbers. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged. However, LSTM networks can still suffer from the exploding gradient problem. In a next iteration of this analysis we could eplore integrating LSTM or GRU methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
